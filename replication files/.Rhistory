foo <- foo[, c(6:8, 11:16, 99, 50, 114, 49, 63, 136, 109, 126, 48, 160, 142, 10)]
# remove 2 rows with missing data
foo <- foo[c(-19, -47), ]
#PREDICTOR MEANS
mwartype <- mean(foo$wartype)
mlogcost <- mean(foo$logcost)
mwardur <- mean(foo$wardur)
mfactnum <- mean(foo$factnum)
mfactnum2 <- mean(foo$factnum2)
mtrnsfcap <- mean(foo$trnsfcap)
mtreaty <- mean(foo$treaty)
mdevelop <- mean(foo$develop)
mexp <- mean(foo$exp)
mdecade <- mean(foo$decade)
muntype4 <- mean(foo$untype4)
mpbs2s3 <- mean(foo$pbs2s3)
#Original Model and effects
glm1 <- glm(pbs2s3 ~ wartype + logcost + wardur + factnum + factnum2 + trnsfcap + treaty
+ develop + exp +untype4+ decade, family = binomial, data = foo)
effect1 <- c()
foonames <- c("wartype", "logcost", "wardur", "factnum", "factnum2", "trnsfcap", "treaty"
,"develop", "exp", "untype4", "decade")
#loop through 315 to calculate effect for each wardur
for(i in 1:315){
control=data.frame(mwartype,mlogcost,i,mfactnum,mfactnum2,mtrnsfcap,mtreaty,mdevelop,mexp,0,mdecade)
treatment=data.frame(mwartype,mlogcost,i,mfactnum,mfactnum2,mtrnsfcap,mtreaty,mdevelop,mexp,1,mdecade)
names(control) <- foonames
names(treatment) <- foonames
effect1[i]<- inv.logit(predict(glm1, newdata = treatment))-inv.logit(predict(glm1, newdata=control))
}
#King and Zeng Model with Interaction term
glm2 <- glm(pbs2s3 ~ wartype + logcost + wardur + factnum + factnum2 + trnsfcap + treaty
+ develop + exp +untype4+ decade+ I(untype4*wardur), family = binomial, data = foo)
effect2 <- c()
#loop through 315 to calculate effect for each wardur
for(i in 1:315){
control=data.frame(mwartype,mlogcost,i,mfactnum,mfactnum2,mtrnsfcap,mtreaty,mdevelop,mexp,0,mdecade,muntype4*mwardur )
treatment=data.frame(mwartype,mlogcost,i,mfactnum,mfactnum2,mtrnsfcap,mtreaty,mdevelop,mexp,1,mdecade,muntype4*mwardur )
names(control) <- foonames
names(treatment) <- foonames
effect2[i]<- inv.logit(predict(glm2, newdata = treatment))-inv.logit(predict(glm2, newdata=control))
}
#plott effects for each wardur
plot(x = c(1:315), y = effect1, type = "l", lty="dotted", lwd=3,
xlim = c(0,315),
ylim = c(0,0.8),
xlab = "Duration of wars in months",
ylab = "Marginal effects of UN peacekeeping operations")
lines(x = c(1:315), y = effect2, type = "l", lwd=3)
text(locator(), labels = c("Model with interaction term","Dotted: Original Model"))
###(3) I assume we use the unformatted
foo <- read.csv("https://course-resources.minerva.kgi.edu/uploaded_files/mke/00086677-3767/peace.csv")
Tr <- rep(0, length(foo$untype))
Tr[which(foo$untype != "None")] <- 1
###(4)
# extract same independent as previous and new dependents PBS2L and PBS5L
foo <- foo[, c(6:8, 11:16, 34:35, 50, 114, 49, 63, 136, 109, 126, 48, 160, 142, 10)]
# add Tr to our dataset
foo <- cbind(foo, Tr)
#remove rows with NA
foo <- foo[complete.cases(foo),]
#logistic regression, using no interaction term, but using Tr
#pbs2l
logistic1 <- glm(pbs2l ~ wartype + logcost + wardur + factnum + factnum2 + trnsfcap + treaty
+ develop + exp + decade +Tr, family = binomial, data = foo)
#pbs5l
logistic2 <- glm(pbs5l ~ wartype + logcost + wardur + factnum + factnum2 + trnsfcap + treaty
+ develop + exp + decade + Tr, family = binomial, data = foo)
#treatmenteffects
te_pbs2l <- mean(inv.logit(predict(logistic1, newdata=subset(foo, Tr==1)))) - mean(inv.logit(predict(logistic1, newdata=subset(foo, Tr==0))))
te_pbs5l <- mean(inv.logit(predict(logistic2, newdata=subset(foo, Tr==1)))) - mean(inv.logit(predict(logistic2, newdata=subset(foo, Tr==0))))
#propensity score matching and balance test
set.seed(419)
pscore <- glm(Tr~wartype + logcost + wardur + factnum + factnum2 + trnsfcap + treaty
+ develop + exp + decade,  family = binomial, data=foo)
mout_pscore2ba <- Match(Y=foo$pbs2l,Tr=foo$Tr, X=pscore$fitted, estimand = "ATT", BiasAdjust = TRUE)
mout_pscore5ba <- Match(Y=foo$pbs5l,Tr=foo$Tr, X=pscore$fitted, estimand = "ATT", BiasAdjust = TRUE)
set.seed(419)
mb_ps_2  <- MatchBalance(Tr~wartype + logcost + wardur + factnum + factnum2 + trnsfcap + treaty
+ develop + exp + decade, data=foo, match.out = mout_pscore2ba, nboots=500)
mb_ps_5  <- MatchBalance(Tr~wartype + logcost + wardur + factnum + factnum2 + trnsfcap + treaty
+ develop + exp + decade, data=foo, match.out = mout_pscore5ba, nboots=500)
#find treatement effects
summary(mout_pscore2ba, full=TRUE)
summary(mout_pscore5ba, full=TRUE)
#genetic matching
X = cbind(foo$wartype,foo$logcost,foo$wardur,foo$factnum,foo$factnum2,foo$trnsfcap
,foo$treaty,foo$develop,foo$exp,foo$decade)
#about 10-20 secs to run
set.seed(419)
genout <- GenMatch(Tr=foo$Tr,X=X, estimand="ATT", pop.size=1000, max.generations=25)
genout <- GenMatch(Tr=foo$Tr,X=X, estimand="ATT", pop.size=1500, max.generations=25)
mout_gen2 <- Match(Y=foo$pbs2l,Tr=foo$Tr, X=X,estimand="ATT",Weight.matrix = genout, BiasAdjust = TRUE)
mout_gen5 <- Match(Y=foo$pbs5l,Tr=foo$Tr, X=X, estimand="ATT", Weight.matrix = genout, BiasAdjust = TRUE)
mb_gen <- MatchBalance(Tr~wartype + logcost + wardur + factnum + factnum2 + trnsfcap + treaty
+ develop + exp + decade, data=foo, match.out = mout_gen2, nboots=500)
#find treatment effects
summary(mout_gen2, full=TRUE)
summary(mout_gen5, full=TRUE)
install.packages("synth")
install.packages("Synth")
?Synth()
?Synth
# load data
data(synth.data)
# load data
data(synth.data)
## First Example: Toy panel dataset
library("Synth")
# load data
data(synth.data)
# create matrices from panel data that provide inputs for synth()
dataprep.out<-
dataprep(
foo = synth.data,
predictors = c("X1", "X2", "X3"),
predictors.op = "mean",
dependent = "Y",
unit.variable = "unit.num",
time.variable = "year",
special.predictors = list(
list("Y", 1991, "mean"),
list("Y", 1985, "mean"),
list("Y", 1980, "mean")
),
treatment.identifier = 7,
controls.identifier = c(29, 2, 13, 17, 32, 38),
time.predictors.prior = c(1984:1989),
time.optimize.ssr = c(1984:1990),
unit.names.variable = "name",
time.plot = 1984:1996
)
View(synth.data)
## run the synth command to identify the weights
## that create the best possible synthetic
## control unit for the treated.
synth.out <- synth(dataprep.out)
## there are two ways to summarize the results
## we can either access the output from synth.out directly
round(synth.out$solution.w,2)
# contains the unit weights or
synth.out$solution.v
## the output from synth opt
## can be flexibly combined with
## the output from dataprep to
## compute other quantities of interest
## for example, the period by period
## discrepancies between the
## treated unit and its synthetic control unit
## can be computed by typing
gaps<- dataprep.out$Y1plot-(
dataprep.out$Y0plot%*%synth.out$solution.w
) ; gaps
## also there are three convenience functions to summarize results.
## to get summary tables for all information
## (V and W weights plus balance btw.
## treated and synthetic control) use the
## synth.tab() command
synth.tables <- synth.tab(
dataprep.res = dataprep.out,
synth.res = synth.out)
print(synth.tables)
## plot in levels (treated and synthetic)
path.plot(dataprep.res = dataprep.out,synth.res = synth.out)
## plot the gaps (treated - synthetic)
gaps.plot(dataprep.res = dataprep.out,synth.res = synth.out)
## First Example: Toy panel dataset
library("Synth")
?synth
data(basque)
# dataprep: prepare data for synth
dataprep.out <-
dataprep(
foo = basque
,predictors= c("school.illit",
"school.prim",
"school.med",
"school.high",
"school.post.high"
,"invest"
)
,predictors.op = c("mean")
,dependent     = c("gdpcap")
,unit.variable = c("regionno")
,time.variable = c("year")
,special.predictors = list(
list("gdpcap",1960:1969,c("mean")),
list("sec.agriculture",seq(1961,1969,2),c("mean")),
list("sec.energy",seq(1961,1969,2),c("mean")),
list("sec.industry",seq(1961,1969,2),c("mean")),
list("sec.construction",seq(1961,1969,2),c("mean")),
list("sec.services.venta",seq(1961,1969,2),c("mean")),
list("sec.services.nonventa",seq(1961,1969,2),c("mean")),
list("popdens",1969,c("mean")))
,treatment.identifier  = 17
,controls.identifier   = c(2:16,18)
,time.predictors.prior = c(1964:1969)
,time.optimize.ssr     = c(1960:1969)
,unit.names.variable   = c("regionname")
,time.plot            = c(1955:1997)
)
names(basque)
basque$regionno
basque$regionname
View(basque)
# dataprep: prepare data for synth
dataprep.out <-
dataprep(
foo = basque
,predictors= c("school.illit",
"school.prim",
"school.med",
"school.high",
"school.post.high"
,"invest"
)
,predictors.op = c("mean")
,dependent     = c("gdpcap")
,unit.variable = c("regionno")
,time.variable = c("year")
,special.predictors = list(
list("gdpcap",1960:1969,c("mean")),
list("sec.agriculture",seq(1961,1969,2),c("mean")),
list("sec.energy",seq(1961,1969,2),c("mean")),
list("sec.industry",seq(1961,1969,2),c("mean")),
list("sec.construction",seq(1961,1969,2),c("mean")),
list("sec.services.venta",seq(1961,1969,2),c("mean")),
list("sec.services.nonventa",seq(1961,1969,2),c("mean")),
list("popdens",1969,c("mean")))
,treatment.identifier  = 15
,controls.identifier   = c(2:14,16:18)
,time.predictors.prior = c(1964:1969)
,time.optimize.ssr     = c(1960:1969)
,unit.names.variable   = c("regionname")
,time.plot            = c(1955:1997)
)
# 1. combine highest and second highest
# schooling category and eliminate highest category
dataprep.out$X1["school.high",] <-
dataprep.out$X1["school.high",] +
dataprep.out$X1["school.post.high",]
dataprep.out$X1                 <-
as.matrix(dataprep.out$X1[
-which(rownames(dataprep.out$X1)=="school.post.high"),])
dataprep.out$X0["school.high",] <-
dataprep.out$X0["school.high",] +
dataprep.out$X0["school.post.high",]
dataprep.out$X0                 <-
dataprep.out$X0[
-which(rownames(dataprep.out$X0)=="school.post.high"),]
# 2. make total and compute shares for the schooling catgeories
lowest  <- which(rownames(dataprep.out$X0)=="school.illit")
highest <- which(rownames(dataprep.out$X0)=="school.high")
dataprep.out$X1[lowest:highest,] <-
(100 * dataprep.out$X1[lowest:highest,]) /
sum(dataprep.out$X1[lowest:highest,])
dataprep.out$X0[lowest:highest,] <-
100 * scale(dataprep.out$X0[lowest:highest,],
center=FALSE,
scale=colSums(dataprep.out$X0[lowest:highest,])
)
# run synth
synth.out <- synth(data.prep.obj = dataprep.out)
# Get result tables
synth.tables <- synth.tab(
dataprep.res = dataprep.out,
synth.res = synth.out
)
# results tables:
print(synth.tables)
# plot results:
# path
path.plot(synth.res = synth.out,
dataprep.res = dataprep.out,
Ylab = c("real per-capita GDP (1986 USD, thousand)"),
Xlab = c("year"),
Ylim = c(0,13),
Legend = c("Basque country","synthetic Basque country"),
)
## gaps
gaps.plot(synth.res = synth.out,
dataprep.res = dataprep.out,
Ylab = c("gap in real per-capita GDP (1986 USD, thousand)"),
Xlab = c("year"),
Ylim = c(-1.5,1.5),
)
# plot results:
# path
path.plot(synth.res = synth.out,
dataprep.res = dataprep.out,
Ylab = c("real per-capita GDP (1986 USD, thousand)"),
Xlab = c("year"),
Ylim = c(0,13),
Legend = c("Murcia country","synthetic Murcia country"),
)
# dataprep: prepare data for synth
dataprep.out <-
dataprep(
foo = basque
,predictors= c("school.illit",
"school.prim",
"school.med",
"school.high",
"school.post.high"
,"invest"
)
,predictors.op = c("mean")
,dependent     = c("gdpcap")
,unit.variable = c("regionno")
,time.variable = c("year")
,special.predictors = list(
list("gdpcap",1960:1969,c("mean")),
list("sec.agriculture",seq(1961,1969,2),c("mean")),
list("sec.energy",seq(1961,1969,2),c("mean")),
list("sec.industry",seq(1961,1969,2),c("mean")),
list("sec.construction",seq(1961,1969,2),c("mean")),
list("sec.services.venta",seq(1961,1969,2),c("mean")),
list("sec.services.nonventa",seq(1961,1969,2),c("mean")),
list("popdens",1969,c("mean")))
,treatment.identifier  = 15
,controls.identifier   = c(2:9,11:14,16:18)
,time.predictors.prior = c(1964:1969)
,time.optimize.ssr     = c(1960:1969)
,unit.names.variable   = c("regionname")
,time.plot            = c(1955:1997)
)
# 1. combine highest and second highest
# schooling category and eliminate highest category
dataprep.out$X1["school.high",] <-
dataprep.out$X1["school.high",] +
dataprep.out$X1["school.post.high",]
dataprep.out$X1                 <-
as.matrix(dataprep.out$X1[
-which(rownames(dataprep.out$X1)=="school.post.high"),])
dataprep.out$X0["school.high",] <-
dataprep.out$X0["school.high",] +
dataprep.out$X0["school.post.high",]
dataprep.out$X0                 <-
dataprep.out$X0[
-which(rownames(dataprep.out$X0)=="school.post.high"),]
# 2. make total and compute shares for the schooling catgeories
lowest  <- which(rownames(dataprep.out$X0)=="school.illit")
highest <- which(rownames(dataprep.out$X0)=="school.high")
dataprep.out$X1[lowest:highest,] <-
(100 * dataprep.out$X1[lowest:highest,]) /
sum(dataprep.out$X1[lowest:highest,])
dataprep.out$X0[lowest:highest,] <-
100 * scale(dataprep.out$X0[lowest:highest,],
center=FALSE,
scale=colSums(dataprep.out$X0[lowest:highest,])
)
# run synth
synth.out <- synth(data.prep.obj = dataprep.out)
# Get result tables
synth.tables <- synth.tab(
dataprep.res = dataprep.out,
synth.res = synth.out
)
# plot results:
# path
path.plot(synth.res = synth.out,
dataprep.res = dataprep.out,
Ylab = c("real per-capita GDP (1986 USD, thousand)"),
Xlab = c("year"),
Ylim = c(0,13),
Legend = c("Murcia country","synthetic Murcia country"),
)
setwd("~/Minerva Year 2/CS112/Final Project/dataverse_files")
library(foreign)
library(data.table)
install.packages("data.table")
install.packages("sampleSelection")
library(foreign)
library(data.table)
library(sampleSelection)
GDP <- fread("UNdata_GDP.csv")[, V4 := NULL]
?fread()
GDP <- fread("UNdata_GDP.csv")
GDP <- fread("UNdata_GDP.csv")[, V4 := NULL]
View(GDP)
setnames(GDP, "Country or Area", "dis_country")
setnames(GDP, "Year", "dis_year_start")
setnames(GDP, "Value", "gdp")
# Convert to 1995 dollars: http://www.multpl.com/gdp-deflator/table
GDP[, loggdp := log(as.numeric(gdp) * 100 / 75.86)]
GDP[, dis_year_start := as.character(dis_year_start)]
GDP[, gdp := NULL]
GDPPC <- fread("UNdata_GDP_Per_Capita.csv")
setnames(GDPPC, "Country or Area", "dis_country")
setnames(GDPPC, "Year", "dis_year_start")
setnames(GDPPC, "Value", "gdppc")
# Convert to 1995 dollars: http://www.multpl.com/gdp-deflator/table
GDPPC[, loggdppc := log(as.numeric(gdppc) * 100 / 75.86)]
GDPPC[, gdppc := NULL]
GDPPC[, dis_year_start := as.character(dis_year_start)]
GDPPC[, Item := NULL]
setkeyv(GDP, c("dis_country", "dis_year_start"))
setkeyv(GDPPC, c("dis_country", "dis_year_start"))
View(GDPPC)
MM <- merge(GDP, GDPPC, by = c("dis_country", "dis_year_start"))
View(MM)
View(GDPPC)
?merge()
View(MM)
str(MM)
str(MM[,1])
types(MM[,1])
type(MM[,1])
MM[,1]
unique(MM[,1])
unique(GDP[,1])
unique(GDPPC[,1])
dataset <- read.dta("Article for GEC (political economy of disaster damage).dta")
View(dataset)
unique(dataset[,1])
Dataset <- data.table(dataset)
Dataset <- unique(data.table(dataset)[, list(dis_country, dis_year_start, lngdppc, lngdp)],
by = c("dis_country", "dis_year_start"))
dis_quake <- subset(dataset, dis_subevent == "gs")
dis_cyclone <- subset(dataset, dis_subevent == "tc")
dis_flood <- subset(dataset, dis_subevent == "gf")
dis_quake_sub <- dis_quake[, c("dis_country", "dis_year_start",
"lndis_loss_overall_usd1995",
"lnt_gs32_magnitude", "lnprop_t_gs32_magnitude")]
Dis_quake_sub <- data.table(dis_quake_sub)
D_quake <- data.table(dis_quake_sub)
U_quake <- unique(D_quake[!is.na(lnprop_t_gs32_magnitude)], by = c("dis_country"))
Countries_quake <- unique(U_quake, by = c("dis_country"))$dis_country
Balanced_quake <- data.table(expand.grid(Countries_quake, 1980:2008, 0))
setnames(Balanced_quake, "Var1", "dis_country")
setnames(Balanced_quake, "Var2", "dis_year_start")
setnames(Balanced_quake, "Var3", "observed")
Balanced_quake <- cbind(Balanced_quake, U_quake$lnprop_t_gs32_magnitude)
setnames(Balanced_quake, "V2", "lnprop_t_gs32_magnitude")
setkeyv(Balanced_quake, c("dis_country", "dis_year_start"))
setkeyv(D_quake, c("dis_country", "dis_year_start"))
D_quake$lnprop_t_gs32_magnitude <- NULL
M_quake <- merge(Balanced_quake, D_quake, by = c("dis_country", "dis_year_start"), all = TRUE)
M_quake[, observed := !is.na(lndis_loss_overall_usd1995)]
N_quake <- merge(M_quake, Dataset, by = c("dis_country", "dis_year_start"), all = TRUE, allow.cartesian = TRUE)
N_quake <- unique(N_quake, by = c("dis_country", "dis_year_start"))
N_quake[, observed := !is.na(lndis_loss_overall_usd1995)]
N_quake[, dis_year_start := as.character(dis_year_start)]
FM_quake <- merge(N_quake, MM, by = c("dis_country", "dis_year_start"), all.x = TRUE)
FM_quake[, loggdp_final := mean(c(lngdp, loggdp), na.rm = TRUE), by = c("dis_country", "dis_year_start")]
FM_quake[, loggdppc_final := mean(c(lngdppc, loggdppc), na.rm = TRUE), by = c("dis_country", "dis_year_start")]
write.csv(FM_quake, "dis_quakes_final_merge_inflation_corrected.csv")
dis_cyclone_sub <- dis_cyclone[, c("dis_country", "dis_year_start",
"lndis_loss_overall_usd1995",
"lnt3_top_wind_speed", "lnprop_t3_top_wind_speed")]
Dis_cyclone_sub <- data.table(dis_cyclone_sub)
D_cyclone <- data.table(dis_cyclone_sub)
U_cyclone <- unique(D_cyclone[!is.na(lnprop_t3_top_wind_speed)], by = c("dis_country"))
Countries_cyclone <- unique(U_cyclone, by = c("dis_country"))$dis_country
Balanced_cyclone <- data.table(expand.grid(Countries_cyclone, 1980:2008, 0))
setnames(Balanced_cyclone, "Var1", "dis_country")
setnames(Balanced_cyclone, "Var2", "dis_year_start")
setnames(Balanced_cyclone, "Var3", "observed")
Balanced_cyclone <- cbind(Balanced_cyclone, U_cyclone$lnprop_t3_top_wind_speed)
setnames(Balanced_cyclone, "V2", "lnprop_t3_top_wind_speed")
setkeyv(Balanced_cyclone, c("dis_country", "dis_year_start"))
setkeyv(D_cyclone, c("dis_country", "dis_year_start"))
D_cyclone$lnprop_t3_top_wind_speed <- NULL
M_cyclone <- merge(Balanced_cyclone, D_cyclone, by = c("dis_country", "dis_year_start"), all = TRUE)
M_cyclone[, observed := !is.na(lndis_loss_overall_usd1995)]
N_cyclone <- merge(M_cyclone, Dataset, by = c("dis_country", "dis_year_start"), all = TRUE, allow.cartesian = TRUE)
N_cyclone <- unique(N_cyclone, by = c("dis_country", "dis_year_start"))
N_cyclone[, observed := !is.na(lndis_loss_overall_usd1995)]
N_cyclone[, dis_year_start := as.character(dis_year_start)]
FM_cyclone <- merge(N_cyclone, MM, by = c("dis_country", "dis_year_start"), all.x = TRUE)
FM_cyclone[, loggdp_final := mean(c(lngdp, loggdp), na.rm = TRUE), by = c("dis_country", "dis_year_start")]
FM_cyclone[, loggdppc_final := mean(c(lngdppc, loggdppc), na.rm = TRUE), by = c("dis_country", "dis_year_start")]
write.csv(FM_cyclone, "dis_cyclones_final_merge_inflation_corrected.csv")
dis_flood_sub <- dis_flood[, c("dis_country", "dis_year_start",
"lndis_loss_overall_usd1995",
"lnsum_precip_abs_pos", "lnprop_sum_precip_abs_pos")]
Dis_flood_sub <- data.table(dis_flood_sub)
D_flood <- data.table(dis_flood_sub)
U_flood <- unique(D_flood[!is.na(lnprop_sum_precip_abs_pos)], by = c("dis_country"))
Countries_flood <- unique(U_flood, by = c("dis_country"))$dis_country
Balanced_flood <- data.table(expand.grid(Countries_flood, 1980:2008, 0))
setnames(Balanced_flood, "Var1", "dis_country")
setnames(Balanced_flood, "Var2", "dis_year_start")
setnames(Balanced_flood, "Var3", "observed")
Balanced_flood <- cbind(Balanced_flood, U_flood$lnprop_sum_precip_abs_pos)
setnames(Balanced_flood, "V2", "lnprop_sum_precip_abs_pos")
setkeyv(Balanced_flood, c("dis_country", "dis_year_start"))
setkeyv(D_flood, c("dis_country", "dis_year_start"))
D_flood$lnprop_sum_precip_abs_pos <- NULL
M_flood <- merge(Balanced_flood, D_flood, by = c("dis_country", "dis_year_start"), all = TRUE)
M_flood[, observed := !is.na(lndis_loss_overall_usd1995)]
N_flood <- merge(M_flood, Dataset, by = c("dis_country", "dis_year_start"), all = TRUE, allow.cartesian = TRUE)
N_flood <- unique(N_flood, by = c("dis_country", "dis_year_start"))
N_flood[, observed := !is.na(lndis_loss_overall_usd1995)]
N_flood[, dis_year_start := as.character(dis_year_start)]
FM_flood <- merge(N_flood, MM, by = c("dis_country", "dis_year_start"), all.x = TRUE)
FM_flood[, loggdp_final := mean(c(lngdp, loggdp), na.rm = TRUE), by = c("dis_country", "dis_year_start")]
FM_flood[, loggdppc_final := mean(c(lngdppc, loggdppc), na.rm = TRUE), by = c("dis_country", "dis_year_start")]
write.csv(FM_flood, "dis_floods_final_merge_inflation_corrected.csv")
View(dataset)
library(Matching)
##balance estimates of Replication paper, Heckmann corrected
MatchBalance(FM_cyclone)
?MatchBalance
##balance estimates of Replication paper, Heckmann corrected
MatchBalance(Tr=FM_cyclone$observed,data = FM_cyclone)
##balance estimates of Replication paper, Heckmann corrected
MatchBalance(data = FM_cyclone)
View(FM_cyclone)
View(M_cyclone)
View(dataset)
View(dataset)
